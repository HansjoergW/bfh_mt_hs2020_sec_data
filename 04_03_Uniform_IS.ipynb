{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp uniformis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "# stellt sicher, dass beim ver√§ndern der core library diese wieder neu geladen wird\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uniform IS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from bfh_mt_hs2020_sec_data.core import * \n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Union, Set\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import shutil          # provides high level file operations\n",
    "import time            # used to measure execution time\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pivot_selected_folder  = \"D:/data/parq_pivot_select\"\n",
    "all_pivoted_folder = \"D:/data/parq_pivot_split\"\n",
    "all_processed_folder = \"D:/data/parq_processed/\"\n",
    "\n",
    "col_list =    [\"stmt\",\"cik\",\"ticker\", \"adsh\",\"period\",\"filed\", \"form\",\"tag\",\"value\",\"report\", \"line\", \"fp\", \"uom\"]\n",
    "pivot_group = [\"cik\",\"ticker\",\"adsh\",\"form\",\"period\",\"filed\",\"fp\", \"qtrs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.163:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>default</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x242e8497e88>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# init Spark\n",
    "spark = get_spark_session() # Session anlegen\n",
    "spark # display the moste important information of the session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 00_Raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the complete unpivoted dataset - if it is needed for debbuging\n",
    "df_all_selected = spark.read.parquet(all_pivot_selected_folder).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it happens sometimes, that the data could not be associated with a right sheet (bs, is, cf, ..). in this cases, the data can appea under \"UN\"\n",
    "# so if expected information cannot be found in the appropriate statement, we have to look in the un statement\n",
    "un_pivot_value = load_data(all_pivoted_folder, spark, \"UN\", \"value\")\n",
    "un_pivot_pd = un_pivot_value.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5989, 1774)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "un_pivot_pd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_un_values(df_to_merge_into, attr_list):\n",
    "    # add possible columns from un set to  cf data with prefix cpy_\n",
    "    attributes = pivot_group[:] # create copy\n",
    "    attributes.extend(attr_list)\n",
    "\n",
    "    un_prepared = un_pivot_pd[attributes].copy()\n",
    "    un_prepared.rename(columns=lambda x: x  if x in pivot_group else (\"cpy_\" + x), inplace=True)\n",
    "\n",
    "    return pd.merge(df_to_merge_into, un_prepared, how='left', on=pivot_group)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
