{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "# stellt sicher, dass beim ver√§ndern der core library diese wieder neu geladen wird\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# filter_and_partition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from bfh_mt_hs2020_sec_data.core import get_spark_session # initialze spark\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Union, Set\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import shutil          # provides high level file operations\n",
    "import time            # used to measure execution time\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_parquet_folder  = \"D:/data_mt/01_3_all_parquet/\"\n",
    "all_filtered_folder = \"D:/data_mt/02_filtered_parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.163:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>default</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x18edcd57688>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# init Spark\n",
    "spark = get_spark_session() # Session anlegen\n",
    "spark # display the moste important information of the session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01_Load_Whole_Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_data(): \n",
    "    start = time.time()\n",
    "    df_all = spark.read.parquet(all_parquet_folder).cache()\n",
    "    duration = time.time() - start\n",
    "    print(\"duration: \", duration)\n",
    "    return df_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02_Filter_Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_whole_ds(df_all):    \n",
    "    start = time.time()\n",
    "\n",
    "    # only primary financial statement information: stmt is not null\n",
    "    # no custom tags: version NOT LIKE '00%'\n",
    "    # only yearly and quarterly reports: form in ('10-K', '10-Q')\n",
    "    filter_string = \"stmt is not null and version NOT LIKE '00%' and form in ('10-K', '10-Q')\"\n",
    "    df_all_filtered = df_all.where(filter_string)\n",
    "\n",
    "    # only information of companies, that are traded at NASDAQ, NYSE\n",
    "    df_all_cik_exchange = df_all_filtered[['cik','exchange']].distinct() \\\n",
    "        .where(\"exchange in ('NASDAQ','NYSE','NYSE ARCA','NYSE MKT') \").selectExpr(\"cik\", \"1 as cik_select\").distinct().cache()\n",
    "    df_all_filter_complete = df_all_filtered.join(df_all_cik_exchange, 'cik', \"left\").where(\"cik_select == 1\")\n",
    "\n",
    "    # only data for the current period\n",
    "    df_all_filter_complete = df_all_filter_complete.where(\"period == ddate\")\n",
    "\n",
    "    df_all_partioned = df_all_filter_complete.repartition(16,col(\"cik\"), col(\"stmt\"))\n",
    "\n",
    "    df_all_partioned.write.parquet(all_filtered_folder)\n",
    "    duration = time.time() - start\n",
    "    print(\"duration: \", duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 99_Execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(all_filtered_folder,  ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duration:  3.696655035018921\n",
      "duration:  850.8133742809296\n"
     ]
    }
   ],
   "source": [
    "df_all = load_all_data()\n",
    "filter_whole_ds(df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
