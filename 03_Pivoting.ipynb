{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp pivoting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "# stellt sicher, dass beim ver√§ndern der core library diese wieder neu geladen wird\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pivoting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to create uniform datasets for the different primary financial statements (Income Statement, CaschFlow, BalanceSheet, ..) it is easier if the data is converted from its vertical for to a horizontal form. This means that we pivot the values based on the tag.\n",
    "\n",
    "Therfore, for every primary financial statement a separate dataset is created.\n",
    "\n",
    "Statement types\n",
    "\n",
    "- IS: IncomeStatement\n",
    "- CF: CashFlow\n",
    "- BS: BalanceSheet\n",
    "- CI: Comprehensive Income\n",
    "- EQ: Equity\n",
    "- CP: CoverPage\n",
    "- UN: Unclassifiable Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from bfh_mt_hs2020_sec_data.core import get_spark_session # initialze spark\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Union, Set\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import shutil          # provides high level file operations\n",
    "import time            # used to measure execution time\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_filtered_folder        = \"D:/data/parq_filtered\"      # source folder with the prepared parquet file\n",
    "all_pivot_selected_folder  = \"D:/data/parq_pivot_select\"  # target folder which will contain only the needed subset of columns\n",
    "all_pivoted_folder         = \"D:/data/parq_pivot_split\"   # target folder for the different pivoted and separated datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.163:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>default</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1b83e051248>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# init Spark\n",
    "spark = get_spark_session() # Session anlegen\n",
    "spark # display the moste important information of the session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01_Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**only execute if necessary**\n",
    "\n",
    "Loads the and filtered data with all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = spark.read.parquet(all_filtered_folder).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entries:  15_650_848\n",
      "duration:  90.72703671455383\n"
     ]
    }
   ],
   "source": [
    "# Expected Amount of Data\n",
    "# Entries:  15_650_848\n",
    "# duration:  66 sec\n",
    "\n",
    "# load all data into memory\n",
    "start = time.time()\n",
    "print(\"Entries: \", \"{:_}\".format(df_all.count())) # loading all dataset into memory\n",
    "duration = time.time() - start\n",
    "print(\"duration: \", duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02_Select"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**only execute if necessary**\n",
    "\n",
    "Creates a new dataset containing only  the columns that are needed during the next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year\n",
    "df_all_selected = df_all.select([\"stmt\",\"cik\",\"ticker\", \"adsh\",\"period\",\"form\",\"filed\",\"tag\",\"value\",\"report\", \"line\", \"fp\", \"uom\", \"qtrs\",\n",
    "                                year(df_all.period).alias('period_year')]).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13713772\n"
     ]
    }
   ],
   "source": [
    "df_all_selected = df_all_selected.where(\"period_year >= 2012\")\n",
    "print(df_all_selected.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(all_pivot_selected_folder,  ignore_errors=True)\n",
    "df_all_selected.write.parquet(all_pivot_selected_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03_Pivoting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_selected = spark.read.parquet(all_pivot_selected_folder).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entries:  15_650_848\n",
      "duration:  28.721192836761475\n"
     ]
    }
   ],
   "source": [
    "# Expected Amount of Data\n",
    "# Entries:  15_650_848\n",
    "# duration:  23 sec\n",
    "\n",
    "# After removing qtrs 2 & 3\n",
    "# Entries:  11_688_113\n",
    "# duration:  11.602035999298096\n",
    "\n",
    "# load all data into memory\n",
    "start = time.time()\n",
    "print(\"Entries: \", \"{:_}\".format(df_all_selected.count())) # loading all dataset into memory\n",
    "duration = time.time() - start\n",
    "print(\"duration: \", duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if a value is null in the original data, we need to set it to zero, otherwise we cannot distinguish between a value that was or was not present \n",
    "# in the dataset after pivoting the data\n",
    "df_all_selected = df_all_selected.fillna({'value':0.0}) # set null value in value column to 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_attrs = ['value'] # column that contains the value which has to be pivoted\n",
    "\n",
    "def pivot_statement(all_data_df, statement:str, stmtfilter:str):\n",
    "    all_stmt_data = all_data_df.where(\"stmt == '\" + statement + \"'\" + stmtfilter).cache()\n",
    "    \n",
    "    shutil.rmtree(all_pivoted_folder + \"/\" + statement,  ignore_errors=True)\n",
    "    \n",
    "    # it is important to also include the qrts,report and line into grupping. \n",
    "    # - for instance, the IS is often not just for the last quarter, but also for the numbers of quarters since beginning of the fiscal year\n",
    "    # - the same tag can appear multiple times in a repor, e.g. the IS has cash at the beginning and at end of the period\n",
    "    grouped_df = all_stmt_data.groupby([\"cik\",\"ticker\",\"adsh\",\"form\",\"period\",\"filed\", \"fp\", \"qtrs\"])\n",
    "    \n",
    "    for attr in pivot_attrs: \n",
    "        # using max() is not the best approach. generally, a tag is only contained once in a report, but there are excptions\n",
    "        # like the CashCashEquivalentsRestrictedCashAndRestrictedCashEquivalents in the CF statement.\n",
    "        pivoted_df = grouped_df.pivot(\"tag\").max(attr)\n",
    "\n",
    "        # repartition(1), so that only one file is created\n",
    "        pivoted_df.repartition(1).write.parquet(all_pivoted_folder + \"/\" + statement + \"/\" + attr) \n",
    "        \n",
    "    all_stmt_data.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pivot_statements(all_data_df, statements,filters):\n",
    "    start = time.time()\n",
    "    \n",
    "    for stmt,stmtfilter in zip(statements,filters): \n",
    "        print (stmt, end = \"\")\n",
    "        section_start = time.time()\n",
    "        pivot_statement(all_data_df, statement=stmt,stmtfilter = stmtfilter)\n",
    "        section_duration = time.time() - section_start\n",
    "        print (\": \", section_duration)\n",
    "\n",
    "    duration = time.time() - start\n",
    "    print(\"duration: \", duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 99_Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_is_reduced = df_all_selected.where(\"stmt == 'IS' and ((form == '10-K' and qtrs == '4') or (form == '10-Q' and qtrs == '1'))\")\n",
    "      \n",
    "# pivot_statements(df_is_reduced, [\"IS\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# section_start = time.time()\n",
    "# pandas_df = df_is_reduced.toPandas()\n",
    "# duration = time.time() - section_start\n",
    "# print(duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_all_selected.where(\"stmt == 'IS'\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IS:  126.44597816467285\n",
      "CF:  249.06622552871704\n",
      "CP:  15.875993490219116\n",
      "BS:  147.50100946426392\n",
      "CI:  59.849002838134766\n",
      "EQ:  67.31700253486633\n",
      "UN:  57.290998458862305\n",
      "duration:  723.5532269477844\n"
     ]
    }
   ],
   "source": [
    "statements = ['IS','CF','CP','BS','CI','EQ','UN']\n",
    "\n",
    "#in case of the IS statement, we are only interested in the qtrs=4 for a 10Ks and qtrs=1 for a 10Qs\n",
    "filters = [\n",
    "        \" and ((form == '10-K' and qtrs == '4') or (form == '10-Q' and qtrs == '1'))\",\n",
    "        \"\",\n",
    "        \"\",\n",
    "        \"\",\n",
    "        \"\",\n",
    "        \"\",\n",
    "        \"\",\n",
    "]\n",
    "\n",
    "# statements = ['IS']\n",
    "# filters = [\n",
    "#         \" and ((form == '10-K' and qtrs == '4') or (form == '10-Q' and qtrs == '1'))\"\n",
    "# ]\n",
    "\n",
    "pivot_statements(df_all_selected, statements, filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+------+--------------------+----------+----+--------------------+----------+------+----+---+------+----+\n",
      "|stmt|    cik|ticker|                adsh|    period|form|                 tag|     value|report|line| fp|   uom|qtrs|\n",
      "+----+-------+------+--------------------+----------+----+--------------------+----------+------+----+---+------+----+\n",
      "|  EQ|1528849|    RH|0001558370-20-011113|2020-07-31|10-Q|TreasuryStockShar...|     600.0|     6|  21| Q2|shares|   2|\n",
      "|  EQ|1528849|    RH|0001558370-20-011113|2020-07-31|10-Q|TreasuryStockValu...|   72000.0|     6|  18| Q2|   USD|   2|\n",
      "|  EQ|1528849|    RH|0001558370-20-011113|2020-07-31|10-Q|OtherComprehensiv...|  918000.0|     6|  23| Q2|   USD|   2|\n",
      "|  EQ|1528849|    RH|0001558370-20-011113|2020-07-31|10-Q|OtherComprehensiv...| 3290000.0|     6|  23| Q2|   USD|   1|\n",
      "|  EQ|1528849|    RH|0001558370-20-011113|2020-07-31|10-Q|StockIssuedDuring...| 7328000.0|     6|  16| Q2|   USD|   1|\n",
      "|  EQ|1528849|    RH|0001558370-20-011113|2020-07-31|10-Q|StockIssuedDuring...| 8125000.0|     6|  16| Q2|   USD|   2|\n",
      "|  EQ|1528849|    RH|0001558370-20-011113|2020-07-31|10-Q|  StockholdersEquity| 1.28491E8|     6|  10| Q2|   USD|   0|\n",
      "|  EQ|1528849|    RH|0001558370-20-011113|2020-07-31|10-Q|  StockholdersEquity| 1.28491E8|     6|  28| Q2|   USD|   0|\n",
      "|  EQ|1528849|    RH|0001558370-20-011113|2020-07-31|10-Q|AdjustmentsToAddi...| 6755000.0|     6|  12| Q2|   USD|   1|\n",
      "|  EQ|1528849|    RH|0001558370-20-011113|2020-07-31|10-Q|AdjustmentsToAddi...|  1.2476E7|     6|  12| Q2|   USD|   2|\n",
      "|  EQ|1528849|    RH|0001558370-20-011113|2020-07-31|10-Q|TreasuryStockReti...|  100000.0|     6|  20| Q2|   USD|   2|\n",
      "|  EQ|1528849|    RH|0001558370-20-011113|2020-07-31|10-Q|       NetIncomeLoss|  9.8423E7|     6|  22| Q2|   USD|   1|\n",
      "|  EQ|1528849|    RH|0001558370-20-011113|2020-07-31|10-Q|       NetIncomeLoss|  9.5211E7|     6|  22| Q2|   USD|   2|\n",
      "|  BS|1528849|    RH|0001558370-20-011113|2020-07-31|10-Q|  LiabilitiesCurrent|  7.8498E8|     2|  33| Q2|   USD|   0|\n",
      "|  BS|1528849|    RH|0001558370-20-011113|2020-07-31|10-Q|  TreasuryStockValue|    5000.0|     2|  48| Q2|   USD|   0|\n",
      "|  BS|1528849|    RH|0001558370-20-011113|2020-07-31|10-Q|PrepaidExpenseAnd...|  6.0497E7|     2|  17| Q2|   USD|   0|\n",
      "|  BS|1528849|    RH|0001558370-20-011113|2020-07-31|10-Q|IntangibleAssetsN...|  6.6863E7|     2|  22| Q2|   USD|   0|\n",
      "|  BS|1528849|    RH|0001558370-20-011113|2020-07-31|10-Q|CommonStockParOrS...|    1.0E-4|     3|   5| Q2|   USD|   0|\n",
      "|  BS|1528849|    RH|0001558370-20-011113|2020-07-31|10-Q|         Liabilities|2.377918E9|     2|  40| Q2|   USD|   0|\n",
      "|  BS|1528849|    RH|0001558370-20-011113|2020-07-31|10-Q|FinanceLeaseLiabi...| 4.92136E8|     2|  38| Q2|   USD|   0|\n",
      "+----+-------+------+--------------------+----------+----+--------------------+----------+------+----+---+------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "just_selected = df_all_selected.where(\"adsh = '0001558370-20-011113'\").cache()\n",
    "just_selected.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "just_is_sel = just_selected.where(\"stmt == 'IS'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = just_is_sel.groupby([\"cik\",\"ticker\",\"adsh\",\"form\",\"period\",\"fp\", \"qtrs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivoted_df = grouped_df.pivot(\"tag\").max('value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd = pivoted_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cik</th>\n",
       "      <th>ticker</th>\n",
       "      <th>adsh</th>\n",
       "      <th>form</th>\n",
       "      <th>period</th>\n",
       "      <th>fp</th>\n",
       "      <th>qtrs</th>\n",
       "      <th>CostOfGoodsAndServicesSold</th>\n",
       "      <th>EarningsPerShareBasic</th>\n",
       "      <th>EarningsPerShareDiluted</th>\n",
       "      <th>...</th>\n",
       "      <th>IncomeLossFromContinuingOperationsBeforeIncomeTaxesExtraordinaryItemsNoncontrollingInterest</th>\n",
       "      <th>IncomeTaxExpenseBenefit</th>\n",
       "      <th>InterestExpense</th>\n",
       "      <th>NetIncomeLoss</th>\n",
       "      <th>OperatingIncomeLoss</th>\n",
       "      <th>OtherExpenses</th>\n",
       "      <th>RevenueFromContractWithCustomerIncludingAssessedTax</th>\n",
       "      <th>SellingGeneralAndAdministrativeExpense</th>\n",
       "      <th>WeightedAverageNumberOfDilutedSharesOutstanding</th>\n",
       "      <th>WeightedAverageNumberOfSharesOutstandingBasic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1528849</td>\n",
       "      <td>RH</td>\n",
       "      <td>0001558370-20-011113</td>\n",
       "      <td>10-Q</td>\n",
       "      <td>2020-07-31</td>\n",
       "      <td>Q2</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1528849</td>\n",
       "      <td>RH</td>\n",
       "      <td>0001558370-20-011113</td>\n",
       "      <td>10-Q</td>\n",
       "      <td>2020-07-31</td>\n",
       "      <td>Q2</td>\n",
       "      <td>1</td>\n",
       "      <td>376863000.0</td>\n",
       "      <td>5.08</td>\n",
       "      <td>3.71</td>\n",
       "      <td>...</td>\n",
       "      <td>117302000.0</td>\n",
       "      <td>18879000.0</td>\n",
       "      <td>19418000.0</td>\n",
       "      <td>98423000.0</td>\n",
       "      <td>136568000.0</td>\n",
       "      <td>19266000.0</td>\n",
       "      <td>7.092820e+08</td>\n",
       "      <td>195851000.0</td>\n",
       "      <td>26564705.0</td>\n",
       "      <td>19386115.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1528849</td>\n",
       "      <td>RH</td>\n",
       "      <td>0001558370-20-011113</td>\n",
       "      <td>10-Q</td>\n",
       "      <td>2020-07-31</td>\n",
       "      <td>Q2</td>\n",
       "      <td>2</td>\n",
       "      <td>660104000.0</td>\n",
       "      <td>4.93</td>\n",
       "      <td>3.75</td>\n",
       "      <td>...</td>\n",
       "      <td>112667000.0</td>\n",
       "      <td>17456000.0</td>\n",
       "      <td>39047000.0</td>\n",
       "      <td>95211000.0</td>\n",
       "      <td>172021000.0</td>\n",
       "      <td>59354000.0</td>\n",
       "      <td>1.192177e+09</td>\n",
       "      <td>360052000.0</td>\n",
       "      <td>25383730.0</td>\n",
       "      <td>19314479.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows √ó 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       cik ticker                  adsh  form      period  fp qtrs  \\\n",
       "0  1528849     RH  0001558370-20-011113  10-Q  2020-07-31  Q2   17   \n",
       "1  1528849     RH  0001558370-20-011113  10-Q  2020-07-31  Q2    1   \n",
       "2  1528849     RH  0001558370-20-011113  10-Q  2020-07-31  Q2    2   \n",
       "\n",
       "   CostOfGoodsAndServicesSold  EarningsPerShareBasic  EarningsPerShareDiluted  \\\n",
       "0                         NaN                    NaN                      NaN   \n",
       "1                 376863000.0                   5.08                     3.71   \n",
       "2                 660104000.0                   4.93                     3.75   \n",
       "\n",
       "   ...  \\\n",
       "0  ...   \n",
       "1  ...   \n",
       "2  ...   \n",
       "\n",
       "   IncomeLossFromContinuingOperationsBeforeIncomeTaxesExtraordinaryItemsNoncontrollingInterest  \\\n",
       "0                                                NaN                                             \n",
       "1                                        117302000.0                                             \n",
       "2                                        112667000.0                                             \n",
       "\n",
       "   IncomeTaxExpenseBenefit  InterestExpense  NetIncomeLoss  \\\n",
       "0                      NaN              NaN            NaN   \n",
       "1               18879000.0       19418000.0     98423000.0   \n",
       "2               17456000.0       39047000.0     95211000.0   \n",
       "\n",
       "   OperatingIncomeLoss  OtherExpenses  \\\n",
       "0                  NaN            NaN   \n",
       "1          136568000.0     19266000.0   \n",
       "2          172021000.0     59354000.0   \n",
       "\n",
       "   RevenueFromContractWithCustomerIncludingAssessedTax  \\\n",
       "0                                                NaN     \n",
       "1                                       7.092820e+08     \n",
       "2                                       1.192177e+09     \n",
       "\n",
       "   SellingGeneralAndAdministrativeExpense  \\\n",
       "0                                     NaN   \n",
       "1                             195851000.0   \n",
       "2                             360052000.0   \n",
       "\n",
       "   WeightedAverageNumberOfDilutedSharesOutstanding  \\\n",
       "0                                              NaN   \n",
       "1                                       26564705.0   \n",
       "2                                       25383730.0   \n",
       "\n",
       "   WeightedAverageNumberOfSharesOutstandingBasic  \n",
       "0                                            NaN  \n",
       "1                                     19386115.0  \n",
       "2                                     19314479.0  \n",
       "\n",
       "[3 rows x 23 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-418899127bb7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_all_selected\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ieu\\Anaconda3\\envs\\spark\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1302\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m             raise AttributeError(\n\u001b[1;32m-> 1304\u001b[1;33m                 \"'%s' object has no attribute '%s'\" % (self.__class__.__name__, name))\n\u001b[0m\u001b[0;32m   1305\u001b[0m         \u001b[0mjc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1306\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "df_all_selected.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
