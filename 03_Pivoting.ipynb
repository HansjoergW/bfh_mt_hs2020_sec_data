{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp pivoting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "# stellt sicher, dass beim ver√§ndern der core library diese wieder neu geladen wird\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pivoting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to create uniform datasets for the different primary financial statements (Income Statement, CaschFlow, BalanceSheet, ..) it is easier if the data is converted from its vertical for to a horizontal form. This means that we pivot the values based on the tag.\n",
    "\n",
    "Therfore, for every primary financial statement a separate dataset is created.\n",
    "\n",
    "Statement types\n",
    "\n",
    "- IS: IncomeStatement\n",
    "- CF: CashFlow\n",
    "- BS: BalanceSheet\n",
    "- CI: Comprehensive Income\n",
    "- EQ: Equity\n",
    "- CP: CoverPage\n",
    "- UN: Unclassifiable Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from bfh_mt_hs2020_sec_data.core import get_spark_session # initialze spark\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Union, Set\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import shutil          # provides high level file operations\n",
    "import time            # used to measure execution time\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_filtered_folder        = \"D:/data/parq_filtered\"      # source folder with the prepared parquet file\n",
    "all_pivot_selected_folder  = \"D:/data/parq_pivot_select\"  # target folder which will contain only the needed subset of columns\n",
    "all_pivoted_folder         = \"D:/data/parq_pivot_split\"   # target folder for the different pivoted and separated datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.163:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>default</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1301a7121c8>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# init Spark\n",
    "spark = get_spark_session() # Session anlegen\n",
    "spark # display the moste important information of the session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01_Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**only execute if necessary**\n",
    "\n",
    "Loads the and filtered data with all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = spark.read.parquet(all_filtered_folder).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entries:  15_650_848\n",
      "duration:  96.77243518829346\n"
     ]
    }
   ],
   "source": [
    "# Expected Amount of Data\n",
    "# Entries:  15_650_848\n",
    "# duration:  66 sec\n",
    "\n",
    "# load all data into memory\n",
    "start = time.time()\n",
    "print(\"Entries: \", \"{:_}\".format(df_all.count())) # loading all dataset into memory\n",
    "duration = time.time() - start\n",
    "print(\"duration: \", duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02_Select"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**only execute if necessary**\n",
    "\n",
    "Creates a new dataset containing only  the columns that are needed during the next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_selected = df_all.select([\"stmt\",\"cik\",\"ticker\", \"adsh\",\"period\",\"form\",\"tag\",\"value\",\"report\", \"line\", \"fp\", \"uom\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(all_pivot_selected_folder,  ignore_errors=True)\n",
    "df_all_selected.write.parquet(all_pivot_selected_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03_Pivoting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_selected = spark.read.parquet(all_pivot_selected_folder).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entries:  15_650_848\n",
      "duration:  16.718958616256714\n"
     ]
    }
   ],
   "source": [
    "# Expected Amount of Data\n",
    "# Entries:  15_650_848\n",
    "# duration:  23 sec\n",
    "\n",
    "# load all data into memory\n",
    "start = time.time()\n",
    "print(\"Entries: \", \"{:_}\".format(df_all_selected.count())) # loading all dataset into memory\n",
    "duration = time.time() - start\n",
    "print(\"duration: \", duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_attrs = ['value'] # column that contains the value which has to be pivoted\n",
    "\n",
    "def pivot_statement(all_data_df, statement:str):\n",
    "    all_stmt_data = all_data_df.where(\"stmt == '\" + statement + \"'\").cache()\n",
    "    \n",
    "    shutil.rmtree(all_pivoted_folder + \"/\" + statement,  ignore_errors=True)\n",
    "    \n",
    "    grouped_df = all_stmt_data.groupby([\"cik\",\"ticker\",\"adsh\",\"form\",\"period\",\"fp\"])\n",
    "    \n",
    "    for attr in pivot_attrs:\n",
    "        \n",
    "        # using max() is not the best approach. generally, a tag is only contained once in a report, but there are excptions\n",
    "        # like the CashCashEquivalentsRestrictedCashAndRestrictedCashEquivalents in the CF statement.\n",
    "        pivoted_df = grouped_df.pivot(\"tag\").max(attr)\n",
    "        \n",
    "        # repartition(1), so that only one file is created\n",
    "        pivoted_df.repartition(1).write.parquet(all_pivoted_folder + \"/\" + statement + \"/\" + attr) \n",
    "        \n",
    "    all_stmt_data.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pivot_statements(all_data_df, statements):\n",
    "    for stmt in statements:\n",
    "        print (stmt)\n",
    "        pivot_statement(all_data_df, statement=stmt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 99_Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#statements = ['IS','CF','CP','BS','CI','EQ','UN']\n",
    "#statements = ['UN']\n",
    "pivot_statements(df_all_selected, statements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
