{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp pivoting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "# stellt sicher, dass beim verändern der core library diese wieder neu geladen wird\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pivoting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um was gehts\n",
    "- Wie sollen die Daten aufbereitet werden, so dass wir möglichst gut ergänzen können?\n",
    "- möghlichst wenig daten?\n",
    "- möglichst alle notwendigen Informationen\n",
    "- möglichst ideal gruppiert\n",
    "\n",
    "Idee:\n",
    "mehrere pivotierte Tabellen (immer pro Statement typ)\n",
    "- 1. mit Werten\n",
    "- 2. mit Statement\n",
    "- 3. mit Zeile im Statement\n",
    "\n",
    "Alle Sätze separat ablegen.. \n",
    "vorerst alle Attribute mitnehmen, die existieren\n",
    "\n",
    "Statement types\n",
    "\n",
    "- IS: IncomeStatement\n",
    "- CF: CashFlow\n",
    "- BS: BalanceSheet\n",
    "- CI: Comprehensive Income\n",
    "- EQ: Equity\n",
    "- CP: CoverPage\n",
    "- UN: Unclassifiable Statement\n",
    "\n",
    "Erwartete  Grössen:\n",
    "-   IS   4559669\n",
    "-   CF   4435322\n",
    "-   BS   4821120\n",
    "-   CI    850503\n",
    "-   EQ    899115\n",
    "-   CP     13408\n",
    "-   UN     71711"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from bfh_mt_hs2020_sec_data.core import get_spark_session # initialze spark\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Union, Set\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import shutil          # provides high level file operations\n",
    "import time            # used to measure execution time\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_filtered_folder        = \"D:/data/parq_filtered\"\n",
    "all_pivot_selected_folder  = \"D:/data/parq_pivot_select\"\n",
    "all_pivoted_folder         = \"D:/data/parq_pivot_split\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.163:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>default</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x25341418208>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# init Spark\n",
    "spark = get_spark_session() # Session anlegen\n",
    "spark # display the moste important information of the session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01_Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**only execute if necessary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = spark.read.parquet(all_filtered_folder).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entries:  15_650_848\n",
      "duration:  96.77243518829346\n"
     ]
    }
   ],
   "source": [
    "# Expected Amount of Data\n",
    "# Entries:  15_650_848\n",
    "# duration:  66 sec\n",
    "\n",
    "# load all data into memory\n",
    "start = time.time()\n",
    "print(\"Entries: \", \"{:_}\".format(df_all.count())) # loading all dataset into memory\n",
    "duration = time.time() - start\n",
    "print(\"duration: \", duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   stmt     count\n",
      "0    IS   4559669\n",
      "1  None  15650848\n",
      "2    UN     71711\n",
      "3    CF   4435322\n",
      "4    CP     13408\n",
      "5    BS   4821120\n",
      "6    CI    850503\n",
      "7    EQ    899115\n"
     ]
    }
   ],
   "source": [
    "p_df_prim_fin_rep_count = df_all.cube('stmt').count().toPandas()\n",
    "print(p_df_prim_fin_rep_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02_Select"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "only keep relevant columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**only execute if necessary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_selected = df_all.select([\"stmt\",\"cik\",\"ticker\", \"adsh\",\"period\",\"form\",\"tag\",\"value\",\"report\", \"line\", \"fp\", \"uom\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(all_pivot_selected_folder,  ignore_errors=True)\n",
    "df_all_selected.write.parquet(all_pivot_selected_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# produziert 1.5 GB CSV Datei unkomprimiert\n",
    "# df_all_selected.toPandas().to_csv(\"./data/pivot_select.csv\",index=False,header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03_Pivoting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_selected = spark.read.parquet(all_pivot_selected_folder).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entries:  15_650_848\n",
      "duration:  12.336964130401611\n"
     ]
    }
   ],
   "source": [
    "# Expected Amount of Data\n",
    "# Entries:  15_650_848\n",
    "# duration:  23 sec\n",
    "\n",
    "# load all data into memory\n",
    "start = time.time()\n",
    "print(\"Entries: \", \"{:_}\".format(df_all_selected.count())) # loading all dataset into memory\n",
    "duration = time.time() - start\n",
    "print(\"duration: \", duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_attrs = ['value', 'report', 'line']\n",
    "\n",
    "def pivot_statement(all_data_df, statement:str):\n",
    "    all_stmt_data = all_data_df.where(\"stmt == '\" + statement + \"'\")\n",
    "    \n",
    "    shutil.rmtree(all_pivoted_folder + \"/\" + stmt,  ignore_errors=True)\n",
    "    \n",
    "    grouped_df = all_stmt_data.groupby([\"cik\",\"ticker\",\"adsh\",\"form\",\"period\",\"fp\"]).cache()\n",
    "    \n",
    "    for attr in pivot_attrs:\n",
    "        pivoted_df = grouped_df.pivot(\"tag\").max(attr)\n",
    "        pivoted_df.repartition(1).write.parquet(all_pivoted_folder + \"/\" + stmt + \"/\" + attr)\n",
    "        \n",
    "    grouped_df.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pivot_statements(all_data_df, statements):\n",
    "    for stmt in statements:\n",
    "        print (stmt)\n",
    "        pivot_statement(all_data_df, stmt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 99_Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#statements = ['IS','CF','CP','BS','CI','EQ','UN']\n",
    "statements = ['']\n",
    "pivot_statements(df_all_selected, statements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
