{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp prepare_whole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "# stellt sicher, dass beim ver√§ndern der core library diese wieder neu geladen wird\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare_Whole_Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'findspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-4574d1179501>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# imports\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mbfh_mt_hs2020_sec_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mget_spark_session\u001b[0m \u001b[1;31m# initialze spark\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpathlib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPath\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mList\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSet\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m  \u001b[1;31m# used to download resources from the web\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ieu\\projects\\bfh_mt_hs2020_sec_data\\bfh_mt_hs2020_sec_data\\core.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Cell\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mfindspark\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_spark_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mappname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"default\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'findspark'"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "from bfh_mt_hs2020_sec_data.core import get_spark_session # initialze spark\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Union, Set\n",
    "import urllib.request  # used to download resources from the web \n",
    "import shutil          # provides high level file operations\n",
    "import time            # used to measure execution time\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType, DateType, DoubleType, BooleanType\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.dataframe import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Definitions\n",
    "all_zip_folder = \"d:/data/sec_zips/\"\n",
    "target_csv_folder = \"d:/data/zip_joined/\"\n",
    "extract_temp_folder = \"d:/data/tmp/\"\n",
    "all_parquet_folder = \"d:/data/parquet/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(all_zip_folder).mkdir(parents=True, exist_ok=True)\n",
    "Path(target_csv_folder).mkdir(parents=True, exist_ok=True)\n",
    "Path(extract_temp_folder).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.163:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>default</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1f04b207d08>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# init Spark\n",
    "spark = get_spark_session() # Session anlegen\n",
    "spark # display the moste important information of the session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01_Download_ZIP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare download urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definitions to create download urls\n",
    "sec_base_path = \"https://www.sec.gov/files/dera/data/financial-statement-data-sets/\"\n",
    "start_year = 2009        # start year to download the data\n",
    "end_year   = 2020        # end year for download\n",
    "format_str = \"{}q{}.zip\" # all file names are like 2020q1.zip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_download_urls_df():\n",
    "    \n",
    "    # create list with all download links\n",
    "    download_urls = []\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        for quarter in range(1,5):\n",
    "            download_urls.append(sec_base_path + format_str.format(year, quarter))\n",
    "            \n",
    "    download_urls.append(\"https://www.sec.gov/files/node/add/data_distribution/2020q1.zip\")\n",
    "    \n",
    "    download_urls_df = spark.createDataFrame(download_urls, StringType())\n",
    "    \n",
    "    download_urls_df = download_urls_df.withColumnRenamed(\"value\",\"url\")\n",
    "    \n",
    "    return download_urls_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downloader_function(url):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "  \n",
    "    # From URL construct the destination path and filename.\n",
    "    file_name = os.path.basename(urllib.parse.urlparse(url).path)\n",
    "    file_path = os.path.join(all_zip_folder, file_name) \n",
    "\n",
    "    # Check if the file has already been downloaded.\n",
    "    if os.path.exists(file_path):\n",
    "        return \"already downloaded\"\n",
    "\n",
    "    # Download and write to file.\n",
    "    try:\n",
    "        with urllib.request.urlopen(url, timeout=30) as urldata,\\\n",
    "              open(file_path, 'wb') as out_file:\n",
    "            shutil.copyfileobj(urldata, out_file)\n",
    "            return \"success\"\n",
    "    except Exception as ex:\n",
    "        return \"failed: {}\".format(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downloader_udf = udf(lambda s: downloader_function(s), StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_zip_files():\n",
    "    download_urls_df = create_download_urls_df()\n",
    "    start_time = time.time()\n",
    "    result_df =  download_urls_df.select('url', downloader_udf('url').alias('result')).collect()\n",
    "    execution_time = (time.time() - start_time)\n",
    "    print(\"execution time:      \", execution_time)\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02_join sec data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants for the names of the filese inside the zip file\n",
    "SUB_TXT = \"sub.txt\"\n",
    "PRE_TXT = \"pre.txt\"\n",
    "NUM_TXT = \"num.txt\"\n",
    "TAG_TXT = \"tag.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list with paths to all the zip files\n",
    "all_zip_path = Path(all_zip_folder)\n",
    "zip_files = [str(file) for file in all_zip_path.glob(\"*.zip\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_in_zip_into_df_extract(zip_file: str, data_file: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "       Extracts the data from zipfile and stores it on disk. \n",
    "       Uses spark.csv.read to read the data into the df\n",
    "    \"\"\"\n",
    "    with zipfile.ZipFile(zip_file, \"r\") as container_zip:\n",
    "        with container_zip.open(data_file) as f:\n",
    "            # create a unique tempfile to extract the data\n",
    "            tempfile = extract_temp_folder +Path(zip_file).name.replace(\".zip\",\"\").replace(\"/\",\"\").replace(\"\\\\\",\"\")+\"_\"+data_file\n",
    "            \n",
    "            with open(tempfile, \"wb+\") as f_temp:\n",
    "                data = f.read()\n",
    "                f_temp.write(data)\n",
    "                f_temp.close()\n",
    "                f_temp_dbfs  = tempfile.replace(\"/dbfs\",\"\")\n",
    "         \n",
    "                df = spark.read.csv(f_temp_dbfs, sep='\\t', header=True)\n",
    "                return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_files(zip_file: str, target_folder: str) -> str:\n",
    "    \"\"\"\n",
    "        Joins the content of the 3 csv files that are contained in the provided zip_file and \n",
    "        create one csv file containing all relevant columns inside target_folder.\n",
    "    \"\"\"\n",
    "    \n",
    "    target_path = target_folder + Path(zip_file).name.replace(\".zip\",\"\").replace(\"/\",\"\").replace(\"\\\\\",\"\")\n",
    "    \n",
    "    if os.path.exists(target_path):\n",
    "        return zip_file + \" : \" + \" already Joined\"\n",
    "    \n",
    "    df_sub = read_csv_in_zip_into_df_extract(zip_file, SUB_TXT)\n",
    "    df_pre = read_csv_in_zip_into_df_extract(zip_file, PRE_TXT)\n",
    "    df_num = read_csv_in_zip_into_df_extract(zip_file, NUM_TXT)\n",
    "    \n",
    "    df_joined = df_num.join(df_sub, [\"adsh\"]).join(df_pre, [\"adsh\",\"tag\",\"version\"],\"left\")\n",
    "    \n",
    "    target_path  = target_path.replace(\"/dbfs\",\"\")\n",
    "    df_joined.write.csv(target_path, compression=\"gzip\", header=True)\n",
    "    \n",
    "    return target_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_zip_content():\n",
    "    start = time.time()\n",
    "    for file in zip_files:\n",
    "        try: \n",
    "            print(join_files(file, target_csv_folder))\n",
    "        except Exception as ex:\n",
    "            print(\"failed: \", file, str(ex))\n",
    "    duration = time.time() - start\n",
    "    print(\"duration: \", duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03_Merge to single Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_csv_path = Path(target_csv_folder)\n",
    "all_csv_path_list = [x.name for x in all_csv_path.iterdir() if x.is_dir()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([  # num.txt  \\\n",
    "                StructField(\"adsh\", \t StringType(), True), \\\n",
    "                StructField(\"tag\", \t \t StringType(), True), \\\n",
    "                StructField(\"version\", \t StringType(), True), \\\n",
    "                StructField(\"coreg\", \t IntegerType(), True), \\\n",
    "                StructField(\"ddate\", \t DateType(), True), # date \\ \n",
    "                StructField(\"qtrs\", \t StringType(), True), \\\n",
    "                StructField(\"uom\", \t \t StringType(), True), \\\n",
    "                StructField(\"value\", \t DoubleType(), True), \\\n",
    "                StructField(\"footnote\",  StringType(), True), \\\n",
    "                      # sub.txt \\ \n",
    "                StructField(\"cik\", \t \t IntegerType(), True), \\\n",
    "                StructField(\"name\", \t StringType(), True), \\\n",
    "                StructField(\"sic\", \t \t IntegerType(), True), \\\n",
    "                StructField(\"countryba\", StringType(), True), \\\n",
    "                StructField(\"stprba\", \t StringType(), True), \\\n",
    "                StructField(\"cityba\", \t StringType(), True), \\\n",
    "                StructField(\"zipba\", \t StringType(), True), \\\n",
    "                StructField(\"bas1\", \t StringType(), True), \\\n",
    "                StructField(\"bas2\", \t StringType(), True), \\\n",
    "                StructField(\"baph\", \t StringType(), True), \\\n",
    "                StructField(\"countryma\", StringType(), True), \\\n",
    "                StructField(\"stprma\", \t StringType(), True), \\\n",
    "                StructField(\"cityma\", \t StringType(), True), \\\n",
    "                StructField(\"zipma\", \t StringType(), True), \\\n",
    "                StructField(\"mas1\", \t StringType(), True), \\\n",
    "                StructField(\"mas2\", \t StringType(), True), \\\n",
    "                StructField(\"countryinc\",StringType(), True), \\\n",
    "                StructField(\"stprinc\", \t StringType(), True), \\\n",
    "                StructField(\"ein\", \t \t IntegerType(), True), \\\n",
    "                StructField(\"former\", \t StringType(), True), \\\n",
    "                StructField(\"changed\", \t StringType(), True), \\\n",
    "                StructField(\"afs\", \t \t StringType(), True), \\\n",
    "                StructField(\"wksi\", \t IntegerType(), True), \\\n",
    "                StructField(\"fye\", \t     StringType(), True), \\\n",
    "                StructField(\"form\", \t StringType(), True), \\\n",
    "                StructField(\"period\", \t DateType(), True),  # date \\\n",
    "                StructField(\"fy\", \t \t IntegerType(), True), \\\n",
    "                StructField(\"fp\", \t \t StringType(), True), \\\n",
    "                StructField(\"filed\", \t DateType(), True), # date \\\n",
    "                StructField(\"accepted\",  StringType(), True), # datetime \\\n",
    "                StructField(\"prevrpt\", \t IntegerType(), True), \\\n",
    "                StructField(\"detail\", \t IntegerType(), True), \\\n",
    "                StructField(\"instance\",  StringType(), True), \\\n",
    "                StructField(\"nciks\", \t IntegerType(), True), \\\n",
    "                StructField(\"aciks\", \t StringType(), True), \\\n",
    "                      # pre.txt \\\n",
    "                StructField(\"report\", \t IntegerType(), True), \\\n",
    "                StructField(\"line\", \t IntegerType(), True), \\\n",
    "                StructField(\"stmt\", \t StringType(), True), \\\n",
    "                StructField(\"inpth\", \t IntegerType(), True), \\\n",
    "                StructField(\"rfile\", \t StringType(), True), \\\n",
    "                StructField(\"plabel\", \t StringType(), True), \\\n",
    "                StructField(\"negating\",  StringType(), True) \\\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_to_single_parquet():\n",
    "    start = time.time()\n",
    "    df_all = spark.read.csv(target_csv_folder + \"*\", header=True, dateFormat=\"yyyyMMdd\", schema=schema)\n",
    "    \n",
    "    # prepare cik_ticker data\n",
    "    df_cik_ticker = spark.read.csv(\"./data/cik_ticker.csv\", sep=\"|\", header=True)[['CIK','Ticker','Name','Exchange']]\n",
    "    df_cik_ticker = df_cik_ticker.withColumnRenamed('Name', \"name_cik_tic\") \\\n",
    "                                .withColumnRenamed('Ticker', \"ticker\") \\\n",
    "                                .withColumnRenamed('Exchange', \"exchange\") \\\n",
    "                                .withColumn(\"cik\", col(\"CIK\").cast(IntegerType()))\n",
    "    \n",
    "    df_all_join = df_all.join(df_cik_ticker, [\"cik\"], \"left\")\n",
    "    \n",
    "    df_all_join.write.parquet(all_parquet_folder)\n",
    "    \n",
    "    duration = time.time() - start\n",
    "    print(\"duration: \", duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 99_Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "execution time:       11.214993715286255\n",
      "[Row(url='https://www.sec.gov/files/dera/data/financial-statement-data-sets/2009q1.zip', result='already downloaded'), Row(url='https://www.sec.gov/files/dera/data/financial-statement-data-sets/2009q2.zip', result='already downloaded'), Row(url='https://www.sec.gov/files/dera/data/financial-statement-data-sets/2009q3.zip', result='already downloaded'), Row(url='https://www.sec.gov/files/dera/data/financial-statement-data-sets/2009q4.zip', result='already downloaded'), Row(url='https://www.sec.gov/files/dera/data/financial-statement-data-sets/2010q1.zip', result='already downloaded'), Row(url='https://www.sec.gov/files/dera/data/financial-statement-data-sets/2010q2.zip', result='already downloaded'), Row(url='https://www.sec.gov/files/dera/data/financial-statement-data-sets/2010q3.zip', result='already downloaded'), Row(url='https://www.sec.gov/files/dera/data/financial-statement-data-sets/2010q4.zip', result='already downloaded'), Row(url='https://www.sec.gov/files/dera/data/financial-statement-data-sets/2011q1.zip', result='already downloaded'), Row(url='https://www.sec.gov/files/dera/data/financial-statement-data-sets/2011q2.zip', result='already downloaded'), Row(url='https://www.sec.gov/files/dera/data/financial-statement-data-sets/2011q3.zip', result='already downloaded'), Row(url='https://www.sec.gov/files/dera/data/financial-statement-data-sets/2011q4.zip', result='already downloaded'), Row(url='https://www.sec.gov/files/dera/data/financial-statement-data-sets/2012q1.zip', result='already downloaded'), Row(url='https://www.sec.gov/files/dera/data/financial-statement-data-sets/2012q2.zip', result='already downloaded'), Row(url='https://www.sec.gov/files/dera/data/financial-statement-data-sets/2012q3.zip', result='already downloaded'), Row(url='https://www.sec.gov/files/dera/data/financial-statement-data-sets/2012q4.zip', result='already downloaded'), Row(url='https://www.sec.gov/files/dera/data/financial-statement-data-sets/2013q1.zip', result='already downloaded'), Row(url='https://www.sec.gov/files/dera/data/financial-statement-data-sets/2013q2.zip', result='already downloaded'), Row(url='https://www.sec.gov/files/dera/data/financial-statement-data-sets/2013q3.zip', result='already downloaded'), Row(url='https://www.sec.gov/files/dera/data/financial-statement-data-sets/2013q4.zip', result='already downloaded'), Row(url='https://www.sec.gov/files/dera/data/financial-statement-data-sets/2014q1.zip', result='already downloaded'), Row(url='https://www.sec.gov/files/dera/data/financial-statement-data-sets/2014q2.zip', result='already downloaded'), Row(url='https://www.sec.gov/files/dera/data/financial-statement-data-sets/2014q3.zip', result='already downloaded'), Row(url='https://www.sec.gov/files/dera/data/financial-statement-data-sets/2014q4.zip', result='already downloaded'), Row(url='https://www.sec.gov/files/dera/data/financial-statement-data-sets/2015q1.zip', result='already downloaded'), Row(url='https://www.sec.gov/files/dera/data/financial-statement-data-sets/2015q2.zip', result='already downloaded'), Row(url='https://www.sec.gov/files/dera/data/financial-statement-data-sets/2015q3.zip', result='already downloaded'), Row(url='https://www.sec.gov/files/dera/data/financial-statement-data-sets/2015q4.zip', result='already downloaded'), Row(url='https://www.sec.gov/files/dera/data/financial-statement-data-sets/2016q1.zip', result='already downloaded'), Row(url='https://www.sec.gov/files/dera/data/financial-statement-data-sets/2016q2.zip', result='already downloaded'), Row(url='https://www.sec.gov/files/dera/data/financial-statement-data-sets/2016q3.zip', result='already downloaded'), Row(url='https://www.sec.gov/files/dera/data/financial-statement-data-sets/2016q4.zip', result='already downloaded'), Row(url='https://www.sec.gov/files/dera/data/financial-statement-data-sets/2017q1.zip', result='already downloaded'), Row(url='https://www.sec.gov/files/dera/data/financial-statement-data-sets/2017q2.zip', result='already downloaded'), Row(url='https://www.sec.gov/files/dera/data/financial-statement-data-sets/2017q3.zip', result='already downloaded'), Row(url='https://www.sec.gov/files/dera/data/financial-statement-data-sets/2017q4.zip', result='already downloaded'), Row(url='https://www.sec.gov/files/dera/data/financial-statement-data-sets/2018q1.zip', result='already downloaded'), Row(url='https://www.sec.gov/files/dera/data/financial-statement-data-sets/2018q2.zip', result='already downloaded'), Row(url='https://www.sec.gov/files/dera/data/financial-statement-data-sets/2018q3.zip', result='already downloaded'), Row(url='https://www.sec.gov/files/dera/data/financial-statement-data-sets/2018q4.zip', result='already downloaded'), Row(url='https://www.sec.gov/files/dera/data/financial-statement-data-sets/2019q1.zip', result='already downloaded'), Row(url='https://www.sec.gov/files/dera/data/financial-statement-data-sets/2019q2.zip', result='already downloaded'), Row(url='https://www.sec.gov/files/dera/data/financial-statement-data-sets/2019q3.zip', result='already downloaded'), Row(url='https://www.sec.gov/files/dera/data/financial-statement-data-sets/2019q4.zip', result='already downloaded'), Row(url='https://www.sec.gov/files/dera/data/financial-statement-data-sets/2020q1.zip', result='already downloaded'), Row(url='https://www.sec.gov/files/dera/data/financial-statement-data-sets/2020q2.zip', result='already downloaded'), Row(url='https://www.sec.gov/files/dera/data/financial-statement-data-sets/2020q3.zip', result='already downloaded'), Row(url='https://www.sec.gov/files/dera/data/financial-statement-data-sets/2020q4.zip', result='failed: HTTP Error 404: Not Found'), Row(url='https://www.sec.gov/files/node/add/data_distribution/2020q1.zip', result='already downloaded')]\n"
     ]
    }
   ],
   "source": [
    "download_result = download_zip_files()\n",
    "print(download_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\data\\sec_zips\\2009q1.zip :  already Joined\n",
      "d:\\data\\sec_zips\\2009q2.zip :  already Joined\n",
      "d:\\data\\sec_zips\\2009q3.zip :  already Joined\n",
      "d:\\data\\sec_zips\\2009q4.zip :  already Joined\n",
      "d:\\data\\sec_zips\\2010q1.zip :  already Joined\n",
      "d:\\data\\sec_zips\\2010q2.zip :  already Joined\n",
      "d:\\data\\sec_zips\\2010q3.zip :  already Joined\n",
      "d:\\data\\sec_zips\\2010q4.zip :  already Joined\n",
      "d:\\data\\sec_zips\\2011q1.zip :  already Joined\n",
      "d:\\data\\sec_zips\\2011q2.zip :  already Joined\n",
      "d:\\data\\sec_zips\\2011q3.zip :  already Joined\n",
      "d:\\data\\sec_zips\\2011q4.zip :  already Joined\n",
      "d:\\data\\sec_zips\\2012q1.zip :  already Joined\n",
      "d:\\data\\sec_zips\\2012q2.zip :  already Joined\n",
      "d:\\data\\sec_zips\\2012q3.zip :  already Joined\n",
      "d:\\data\\sec_zips\\2012q4.zip :  already Joined\n",
      "d:\\data\\sec_zips\\2013q1.zip :  already Joined\n",
      "d:\\data\\sec_zips\\2013q2.zip :  already Joined\n",
      "d:\\data\\sec_zips\\2013q3.zip :  already Joined\n",
      "d:\\data\\sec_zips\\2013q4.zip :  already Joined\n",
      "d:\\data\\sec_zips\\2014q1.zip :  already Joined\n",
      "d:\\data\\sec_zips\\2014q2.zip :  already Joined\n",
      "d:\\data\\sec_zips\\2014q3.zip :  already Joined\n",
      "d:\\data\\sec_zips\\2014q4.zip :  already Joined\n",
      "d:\\data\\sec_zips\\2015q1.zip :  already Joined\n",
      "d:\\data\\sec_zips\\2015q2.zip :  already Joined\n",
      "d:\\data\\sec_zips\\2015q3.zip :  already Joined\n",
      "d:\\data\\sec_zips\\2015q4.zip :  already Joined\n",
      "d:\\data\\sec_zips\\2016q1.zip :  already Joined\n",
      "d:\\data\\sec_zips\\2016q2.zip :  already Joined\n",
      "d:\\data\\sec_zips\\2016q3.zip :  already Joined\n",
      "d:\\data\\sec_zips\\2016q4.zip :  already Joined\n",
      "d:\\data\\sec_zips\\2017q1.zip :  already Joined\n",
      "d:\\data\\sec_zips\\2017q2.zip :  already Joined\n",
      "d:\\data\\sec_zips\\2017q3.zip :  already Joined\n",
      "d:\\data\\sec_zips\\2017q4.zip :  already Joined\n",
      "d:\\data\\sec_zips\\2018q1.zip :  already Joined\n",
      "d:\\data\\sec_zips\\2018q2.zip :  already Joined\n",
      "d:\\data\\sec_zips\\2018q3.zip :  already Joined\n",
      "d:\\data\\sec_zips\\2018q4.zip :  already Joined\n",
      "d:\\data\\sec_zips\\2019q1.zip :  already Joined\n",
      "d:\\data\\sec_zips\\2019q2.zip :  already Joined\n",
      "d:\\data\\sec_zips\\2019q3.zip :  already Joined\n",
      "d:\\data\\sec_zips\\2019q4.zip :  already Joined\n",
      "d:\\data\\sec_zips\\2020q1.zip :  already Joined\n",
      "d:\\data\\sec_zips\\2020q2.zip :  already Joined\n",
      "d:\\data\\sec_zips\\2020q3.zip :  already Joined\n",
      "duration:  0.01799941062927246\n"
     ]
    }
   ],
   "source": [
    "join_zip_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Code to clear the extract_temp_folder\n",
    "shutil.rmtree(extract_temp_folder)\n",
    "Path(extract_temp_folder).mkdir(parents=True, exist_ok=True) # create directory after it was deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duration:  981.0835425853729\n"
     ]
    }
   ],
   "source": [
    "shutil.rmtree(all_parquet_folder,  ignore_errors=True) # make sure the target folder is empty\n",
    "merge_to_single_parquet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
